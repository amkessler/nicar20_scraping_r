---
title: "nicar_20_scraping_r_pdf"
author: "YHan"
date: "1/27/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(rvest)
library(dplyr)
```

# Read the webpage and get the html
```{r}
webpage <- read_html("https://www.dol.gov/agencies/ebsa/laws-and-regulations/rules-and-regulations/public-comments/1210-AB85")

webpage
```


# Select elements/nodes with html_node() function

inspect element to find the HTML tags
```{r}
#read the html content into R and assigns to webpage object
results <- html_nodes(webpage, 'ol li a')

results
```

## extract attributes hyperlinks
```{r}
urls <- html_nodes(webpage, 'ol li a') %>% html_attr("href")

length(urls)

tail(urls)
```

The last two urls are hyperlinks for petitions not comment letters. They were also pulled because of the same tags "ol li a".

A combination of HTML class and tags can help us pull elements more precisely because the comment letters and petitions have different column width.

```{r}
html_nodes(webpage, '.col-sm-4 ol li a')

urls <- html_nodes(webpage, 'ol li a') %>% html_attr("href") %>% head() #just scraping the first six for demonstration purpose
```

```{r}
urls
```

But you can't download the PDFs from these urls yet, because complete urls start with "https://www.dol.gov".

download the first pdf to the letter folder, using download.file() function
```{r}
download.file("https://www.dol.gov/sites/dolgov/files/EBSA/laws-and-regulations/rules-and-regulations/public-comments/1210-AB85/00001.pdf", "~/Desktop/nicar2020/nicar_2020_scraping_r/letter/1.pdf", mode="wb")
```

We can download multiple files using for loop.
```{r, eval=FALSE}

download.folder = '~/Desktop/nicar2020/nicar_2020_scraping_r/letter/'

for(i in seq_along(urls)){
    pdf.name = paste(download.folder, i, '.pdf', sep = '')
    dol_url = paste("https://www.dol.gov", urls[i], sep = '')
    download.file(dol_url, pdf.name, mode="wb")
}
```

Next you can use pdftools to parse the pdf files
```{r}
#install.packages("pdftools")
library(pdftools)
base_directory<- '~/Desktop/nicar2020/nicar_2020_scraping_r/letter'

file.path(base_directory, "1.pdf") %>% pdf_text()

```

```{r}
library(pdftools)

l1 = c() ##create an empty vector to store the letter numbers
l2 = c() ##create an empty vector to store the text objects
for(i in seq_along(urls)){
    
    pdf.name = paste("~/Desktop/nicar2020/nicar_2020_scraping_r/letter/", i, '.pdf', sep = '') ##file name of the pdf to be extracted
    txt = pdf_text(pdf.name) ##extract the text from the pdf
    out = capture.output(cat(txt)) ## turn the text into readable format
    l2 = c(l2,out) ## add the text object into the list
    l1 = c(l1, rep(i,length(out)))
}

letters = do.call(rbind, Map(data.frame, id=l1,text=l2))
letters
```

Then you can analyze the text, i.e. sentiment analysis, with package Tidyverse.



